{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af27498a-8318-47cb-981b-5a1bf223210e",
   "metadata": {},
   "source": [
    "# M2 DataScience (Part II) - Lab session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a63107-7632-40fa-a59e-7220841aa0f0",
   "metadata": {},
   "source": [
    "It's time to practice all (or at least some of) the notions we mention during the second part of this course.\n",
    "\n",
    "This notebook is split into three (independent) parts: \n",
    "\n",
    "- I. Optimization (not strictly related to machine learning). \n",
    "- II. Classification and Deep Learning on image datasets. \n",
    "- III. Regression and regularization.\n",
    "\n",
    "**Note:** The Lab is a bit long, do not worry if you don't do everything in 4 hours, and you can chose on which part you want to work in priority depending on your interest. \n",
    "\n",
    "Before starting to work, you will need the following packages. Some (most) of them are not native and require to be installed first. You can check if the installation went correctly by running the following code cell. It should not raise error (warnings---such as \"you don't have a GPU\"---are ok). \n",
    "\n",
    "**Note:** After installing a package, you (unfortunately) need to restart the notebook (the loop-arrow on the top horizontal banner). \n",
    "\n",
    "Used package and version (note: it's quite likely that having a similar version should work the same, so do not worry if you have numpy `1.22.2` for instance. But just in case, I provide the versions used when this notebook was designed.\n",
    "- `numpy` version `1.22.4`\n",
    "- `scikit-learn` version `1.1.1` \n",
    "- `matploblib` version `3.5.2`\n",
    "- `jax` version `0.4.1` \n",
    "- `tensorflow` version `2.8.2` \n",
    "\n",
    "**Note:** In case you struggle to install the libraries, you can go for an online version using [Google colab](https://colab.research.google.com/) (requires a gmail account as far as I remember), or [Binder](https://mybinder.org/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ccc7b8-02ad-4b43-bfb9-78027159dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327b0e6-3906-4313-8480-68d2d53dc7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see if you're ready to get started. \n",
    "\n",
    "# numpy : the 101 library for scientific Python\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn : the most well-known library for basic Machine Learning in Python\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Matplotlib : to plot mathematical content in python.\n",
    "import matplotlib.pyplot as plt\n",
    "# A complementary line to use LaTeX with matplotlib. You can try removing it if you have compilation issues. \n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"Helvetica\"\n",
    "})\n",
    "\n",
    "# Jax : a Google developed library that provides auto-diff. \n",
    "# One cool aspect: jax.numpy is a numpy-like API (i.e. you can switch between np.blabla and jnp.blabla faitfully).\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Tensorflow : a Google developed library for neural networks.\n",
    "import tensorflow as tf\n",
    "\n",
    "# A complementary file that provide utilitary functions for the lab.\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2263c2e-f163-4ed7-8b1e-06106f9c2661",
   "metadata": {},
   "source": [
    "_Cells needed to define $\\LaTeX$ macro._\n",
    "\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ddcec-e6b2-4e00-ad8f-5bebc7c0e1c5",
   "metadata": {},
   "source": [
    "# I. Optimization and automatic differentiation with JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78d0a9-c9fd-4829-88ce-6c7b866d069f",
   "metadata": {},
   "source": [
    "We recall (briefly) the basics of gradient descent: to (locally) minimize a function $F : \\R^d \\to \\R$, we start from some (typically random) $x_0$, and then define a sequence \n",
    "\n",
    "$$x_{t+1} = x_t - \\eta \\nabla F(x_t)$$\n",
    "\n",
    "where $\\eta_t$ is a pre-determined parameter (_learning rate_) that may depend on $t$ (or not). Under reasonnable assumptions, $x_t$ should converge toward a local minimizer of $F$ (global if $F$ is convex). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d78eb-ac75-478d-a7f2-791311e4606f",
   "metadata": {},
   "source": [
    "## 1. Computing gradients with JAX. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1387e3f-a3bb-46f2-a349-967c9d525f68",
   "metadata": {},
   "source": [
    "Note in particular that we need to compute $\\nabla F$. While for simple functions, you can do that \"by hand\", things become quickly messy. \n",
    "Thankfully, we will benefit from _Automatic differenciation_ provided (in this lab) by the library `jax`, developped by Alphabet. \n",
    "You can find the complete documentation [here](https://github.com/google/jax). \n",
    "Note that you can do similar things using `PyTorch`. \n",
    "\n",
    "In a nutshell, it works in the following way:\n",
    "- define your variables (at least the one you want to optimize) as `jax.numpy.array`.\n",
    "- define functions |f` using such variables. \n",
    "- You can get gradients of functions just typing `jax.grad(f)`\n",
    "\n",
    "See below for an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc234ba-8cb6-402d-bea7-38295c151660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    '''\n",
    "    Function that takes x \\in \\R^d and returns its squared euclidean norm. \n",
    "    '''\n",
    "    return jnp.linalg.norm(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cea3aa-dad0-49aa-a1bd-9d5270ee6de8",
   "metadata": {},
   "source": [
    "And now we can just _define_ its gradient with jax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec355bf-f307-44fa-aa97-6bda20da12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df = jax.grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce69dbb-fb77-47d6-b910-b70cf28e37d1",
   "metadata": {},
   "source": [
    "The object `Df` is a function, it takes $x$ and return $\\nabla f(x)$. \n",
    "\n",
    "Note however that `jax` always returns arrays, even if they contain a single digit. Note also that it strictly requires `float` as input when computing gradients. That is, write `x = 1.` and not `x = 1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2602a70-e45e-4e5c-9f98-2c3b38fa62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.array([3., 4.])\n",
    "# Compute the value of f at x\n",
    "f(x)  # Should be 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5311eb-dc12-4414-854f-7e89954648f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute its gradient at x. \n",
    "Df(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f861033-45e1-4bca-b2ad-b5a1d8e15e22",
   "metadata": {},
   "source": [
    "`JAX` provides further control. For instance, you may consider functions of several parameters, some of them being your true variable of interest, and the other being more general parameters. For instance, let us consider the following function : \n",
    "\n",
    "$$F : \\R^d \\ni x \\mapsto a \\sin(\\|x\\|^2) + b e^{\\|x\\|^2}.$$\n",
    "\n",
    "Here, $a$ and $b$ shound be considered as fixed parameters, and $x$ is the variable. The natural implementation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b0af6-266e-4b43-95ae-97f975f49a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, b):\n",
    "    nx = x[0]**2 + x[1]**2\n",
    "    return a * jnp.sin(nx) + b * jnp.exp(nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea836e-b2f8-47fd-b1da-5d9b1c748565",
   "metadata": {},
   "source": [
    "Computing the gradient of this function with respect to $x$ by hand is already a bit painful. Let `JAX` do the work. \n",
    "\n",
    "_Note:_ By default, `JAX` computes the gradient with respect to the first variable of `f` (here `x`). If, for some reason, we want the gradient for one or several other parameters, we can provide the keyword `argnums` when calling `jax.grad()`, telling jax the variables with respect to which we want to compute the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada343ec-8940-4ce8-9f63-50f7e0fc4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df = jax.grad(f)  # the gradient of x \\mapsto f(x, a, b)\n",
    "Df_v2 = jax.grad(f, argnums = (1,2))  # the gradient of (a,b) \\mapsto f(x, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b37c0-a2ef-4a2f-92fa-790c931042b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.array([1., 1.])\n",
    "a,b = 2., 3.\n",
    "\n",
    "print(\"Grad with respect to x:\", Df(x, a=a, b=b))\n",
    "print(\"Grad with respect to (a,b):\", Df_v2(x, a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91db0a0-b5af-483c-8567-a59daf5e3bf7",
   "metadata": {},
   "source": [
    "_Remark:_ Be careful about the shapes & cie. The gradient with respect to `x` is of shape `1 x 2`, (we differentiate \"1 variable in dim 2\") but the one with respect to `a,b` is of shape `2 x 1` here (2 variables of dim 1)---we could harmonize stuff by replacing the two variables `a,b` by a single one of dim 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcf8838-ea88-47d3-b6ee-83f5ec9308f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac65c3-f78c-40cd-88ee-c297f71a03ed",
   "metadata": {},
   "source": [
    "**Question 1:** \n",
    "\n",
    "$\\bullet$ Implement the function (using `jnp` to get access to automatic differentiation) \n",
    "\n",
    "$$ F( \\cdot ; \\theta) : \\R^d \\ni x \\mapsto \\|x\\|^2 + 3 \\cdot \\sin(\\theta \\cdot x),$$\n",
    "\n",
    "where $\\theta \\in \\R^d$. \n",
    "\n",
    "$\\bullet$ Define two functions: `DF_x` and `DF_theta`, that respectively compute the gradient of $F$ with respect to $x$ and to $\\theta$. \n",
    "\n",
    "_Indication:_ Scalar product is obtain with `jnp.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45124b73-91f8-4017-909b-05e01ed41ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x, theta):\n",
    "    # Complete the following\n",
    "    ...\n",
    "    \n",
    "DF_x = ...\n",
    "DF_theta = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79da2f07-2691-4228-9bd9-e2f6689012ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7554018b-1e39-4eb9-a135-1361a9109c99",
   "metadata": {},
   "source": [
    "The function $x \\mapsto \\nabla F(x)$ goes from $\\R^2 \\to \\R^2$ is called a _gradient field_. Just for fun, we visualize one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab94fcb-31d9-41cf-8f28-48673103a1a7",
   "metadata": {},
   "source": [
    "**Question 2:** Run the following code (which uses the function `plot_gradient_field` provided by the file `utils.py`), with $\\theta = (1, 1)$. \n",
    "\n",
    "From the plot, can you say if the function $F$ is convex (visually)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536ad8e-6e57-43fd-a1f8-27615f2ea2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_gradient_field(DF_x, theta, num=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263b182-4c6f-465c-8ada-06753018f133",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14aeb81-ccf7-4549-a7ca-0bfe61fc7378",
   "metadata": {},
   "source": [
    "## 2. (Stochastic) Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1e729-8b2e-4e47-abfd-f97f7d0df2ac",
   "metadata": {},
   "source": [
    "It's time to implement the gradient descent (and its stochastic version later). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002053cb-9f85-47bd-af03-ff21912a5452",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe5032-a1d2-45fd-bb52-bdb84cd9e4b3",
   "metadata": {},
   "source": [
    "**Question 3:** Complete the following code to implement a (standard) gradient descent over a function `F`, starting from `x0`, with learning rate `lr`, and run for a fixed number of step `n_step`. \n",
    "\n",
    "For display purpose, the function should return three things:\n",
    "- the list of positions $(x_t)_t$ crossed during the descent. \n",
    "- the list of losses $(F(x_t)_t$\n",
    "- the list of gradient $(\\nabla F(x_t))_t$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f7e41-ca2a-4a2d-9785-5ae511271f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(F, x0, lr, n_step):\n",
    "    \"\"\"\n",
    "    Run the gradient descent algorithm\n",
    "    \n",
    "    :param F: a real-valued function, should be compatible with jax.grad\n",
    "    :param x0: starting point for the GD, should be legal input for F\n",
    "    :param lr: the learning_rate parameter (float)\n",
    "    :param n_step: number of step in the iterative loop\n",
    "    \n",
    "    :returns: lists of positions [x_i], losses [F(x_i)] and gradients [grad(F)(x_i)] encountered during the descent. \n",
    "    \"\"\"\n",
    "    x_current = x0\n",
    "    grad_F = jax.grad(F)\n",
    "    \n",
    "    all_x = [x0]\n",
    "    all_losses = [F(x0)]\n",
    "    all_grad = [grad_F(x0)]\n",
    "    \n",
    "    for t in range(n_step):\n",
    "        # TO COMPLETE\n",
    "        ...\n",
    "        \n",
    "    return all_x, all_losses, all_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e146aeb7-68d6-4c31-9cd1-96605ec0c274",
   "metadata": {},
   "source": [
    "You can test your function on the following example, which uses the function `plot_gd_1d` from `utils.py`, which attempts at minimizing the simple function from $\\R \\to \\R$ defined by $F(x) = \\frac{x^2}{2} + \\sin(x)^4$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275dc98-ffff-45a6-abac-6b2c24d704e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x):\n",
    "    return x**2 / 2 + jnp.sin(x)**4\n",
    "\n",
    "all_x, all_losses, all_grad = gradient_descent(F, x0=3., lr=0.1, n_step=100)\n",
    "utils.plot_gd_1d(F, all_x, all_losses, all_grad, lr=0.1, xs=np.linspace(-4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a6193-b465-42a7-8b5f-f79527ae0ae7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad7373-51b5-4e46-b784-c9e83cec4154",
   "metadata": {},
   "source": [
    "**Question 4:** Run a gradient descent and plot it for the function $x \\mapsto e^{-\\frac{1}{x^2}}$, starting from $x_0 = 1$, with $\\eta = 0.1$ and $T = 100$ steps. What do you observe? What's the reason behind this phenomenon?\n",
    "\n",
    "Same question for the map $x \\mapsto |x|$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77d68d-f0ba-43b0-8b2c-1dbafcedd7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24619647-106f-4c06-a131-59de60677446",
   "metadata": {},
   "source": [
    "-- Write your comment here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b82c07-e62e-4b5f-99a9-ee5c31ed5282",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbac27-504f-4f18-a570-6c79d68bddc6",
   "metadata": {},
   "source": [
    "**Question 5:** Here we propose to run the gradient descent for $T = 100$ steps before stopping. Maybe this is too few (we stop before convergence), or maybe this is too large (we waste computational time). \n",
    "\n",
    "What improvement would you suggest to improve on this arbitrary stopping criterion? Under which assumptions should it work?\n",
    "\n",
    "_Bonus:_ Implement and test your proposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf65042-66d9-4ed8-a80a-fe2fbea280c5",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624f463-e2b8-436d-be86-c65e100a395b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee42b863-5801-4481-936c-03f4fe006ef0",
   "metadata": {},
   "source": [
    "$\\bullet$ Let us now consider a more ML oriented task, hence the stochastic version of the gradient descent. \n",
    "\n",
    "We have data given by observations and labels in $(x,y) \\in \\R \\times \\R$, and we suspect that the relation between $y$ and $x$ is of the form \n",
    "\n",
    "$$y = x \\cdot \\exp\\left({-\\frac{(x-a)^2}{2}}\\right) + \\exp\\left({-\\frac{(x-b)^2}{2}}\\right) + \\epsilon,$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0,\\sigma)$ is some additional noise that we model using a Gaussian distribution. We want to estimate $a,b$ from our sample $(x_i, y_i)_{i=1}^n$. For compactness, we will set $\\theta = (a,b) \\in \\R^2$. \n",
    "\n",
    "We consider a _regression_ task with the mean squared error. \n",
    "Thus, the training step aims at minimizing the empirical risk, a.k.a. loss function\n",
    "\n",
    "$$L : \\theta \\mapsto \\frac{1}{n} \\sum_{i=1}^n \\| y_i - F(x_i ; \\theta)\\|^2,$$\n",
    "\n",
    "with $F(x ; \\theta) = x \\cdot \\exp\\left({-\\frac{(x-a)^2}{2}}\\right) + \\exp\\left({-\\frac{(x-b)^2}{2}}\\right)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2474e3-6deb-4938-be04-5220a937a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: generate and plot the data\n",
    "\n",
    "x_train, y_train = utils.data_generation()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_train, y_train)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba1648-9939-43fb-abad-9e61eafe5b50",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7881e2-37c3-4218-a368-4a1b302f1b1b",
   "metadata": {},
   "source": [
    "**Question 6:** Define a function `F(x,theta)` that computes $F(x ; \\theta)$ with $\\theta = (a,b)$.\n",
    "\n",
    "_Note:_ Make it compatible with JAX synthax, or you will have to compute the gradient by yourself! ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0ed43d-088b-4a68-8518-3ce65b5c3389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x,theta):\n",
    "    # TO COMPLETE\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d506caf-48dd-4854-bdf5-b348fd2ccccc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a445b716-e0a4-4022-83a3-267f2b5b60d2",
   "metadata": {},
   "source": [
    "We recall that the stochastic gradient descent is about producing a sequence of the form \n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta_t \\nabla_\\theta \\left(\\| y_i -  F(x_i, \\theta_t)\\|^2\\right)$$\n",
    "\n",
    "for a _single_ point $(x_i, y_i)$, where $i \\sim \\mathrm{Unif}(1,\\dots,n)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0371d9-f666-43ad-9224-6b2cd04212dc",
   "metadata": {},
   "source": [
    "**Question 7:** Implement the stochastic gradient descent algorithm to minimize $L$. It must return the list of $(\\theta_t)_t$ recorded during the descent. \n",
    "\n",
    "Comment on the following points: \n",
    "- Does your SGD converges? How many steps do you run?\n",
    "- What kind of stopping criterion did/should you use?\n",
    "- Does the output quality depends on the initialization? From instance try starting from $\\theta_0 = (-1, 1)$, and then $\\theta_0 = (1, -1)$. What can you conclude about the objective function $L$?\n",
    "\n",
    "_Indication:_ to sample a uniform index $i$ between $0$ and $n-1$,  you can use `np.random.randint(n)`. \n",
    "\n",
    "_Note:_ This question is a bit open, think about everything you need to implement the SGD, including bonus options to optimize your algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3b1b09-cca8-4308-8d1f-bb069dcd6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(F, x_train, y_train, theta_0, lr, n_step):\n",
    "    \"\"\"\n",
    "    Run the *stochastic* gradient descent algorithm\n",
    "    \n",
    "    :param F: a real-valued function, should be compatible with jax.grad. It takes two arguments (x,theta), and we consider its derivative w.r.t. theta. \n",
    "    :param x_train: list (or other iterable) of training observations. \n",
    "    :param y_train: list (or other iterable) of training labels. \n",
    "    :param theta_0: starting point for the SGD, should be legal input for F\n",
    "    :param lr: the learning_rate parameter (float)\n",
    "    :param n_step: number of step in the iterative loop\n",
    "    \n",
    "    :returns: lists of positions [theta_i] encountered during the descent. \n",
    "    \"\"\"\n",
    "    # WRITE YOUR CODE HERE...\n",
    "    ...\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3a9d3a-13e4-4f1a-9ba7-612a8d6b8c25",
   "metadata": {},
   "source": [
    "You can run the following cell to check the behavior of your code (it takes `thetas`, the output of your `sgd`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e6952-bf53-4d02-8ed4-bd5a812fa7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = sgd(F, x_train, y_train, theta_0 = jnp.array([-1, 1.]), lr=0.1, n_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ddda16-18e7-418a-bbd3-8a967ed89f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_sgd(F, x_train, y_train, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a29e195-411d-46ab-8e04-a826b643ef24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f5159-46a0-46d9-926a-b1e2e5d416b2",
   "metadata": {},
   "source": [
    "# II. Classification and Deep Learning in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e2496-7882-4e70-8884-fc0fc26723f8",
   "metadata": {},
   "source": [
    "This part is dedicated to a \"real\" ML task: classification of images. We are given (reasonnably small, but not that small) sets of images that belong to (10) different classes. \n",
    "\n",
    "Our goal is to maximize the _test accuracy_ (i.e. proportion of correct predictions by the model on unseen data). As explained in the lectures, this is done by minimizing the _cross entropy_. \n",
    "\n",
    "We will considers models as being neural networks, implemented in `tensorflow` (those motivated can try to do the same in `pyTorch` and check the differences). \n",
    "One good aspect of `tensorflow` for Deep Learning is that it provides a simple and efficient API (built from `keras`) to design neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a704c09d-a373-478c-bc62-8d70371c9dc3",
   "metadata": {},
   "source": [
    "## 1. The MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521657a8-4e16-406f-82b4-69ed0efd53d2",
   "metadata": {},
   "source": [
    "This is probably the most celebrated image dataset for machine learning. It's a toy dataset made of greyscale images of size $28 \\times 28$---that is an image is an `array` of shape `28,28` filled with integer between `0` and `255`---, representing handwritten digits (between $0$ and $9$), which are the labels corresponding to an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a260fee-7d15-4efe-aef7-7b2e5be31482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize to make value between 0 and 1. Not necessary, but common practice to avoid some numerical effects.\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d9bab-e50e-4595-bcbf-9fb2794c3580",
   "metadata": {},
   "source": [
    "Let's first investigate the dataset and do some visualization to fix the ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f65a78-f4b3-49d6-a2b8-89b62b88951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the training set:\", x_train.shape)\n",
    "print(\"Small snapshot of training labels:\", y_train[:10])\n",
    "print(\"Shape of the test set:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d5378-ebc6-4626-b4cc-70439aa6ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(x_train.shape[0])  # sample a random index in the dataset\n",
    "im = x_train[index]  # select the corresponding observation\n",
    "label = y_train[index]  # select the corresponding label\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(im, cmap='Greys')  # plot the image\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Image representing a %s\" %label, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d748da-f397-4630-8bcf-64d62b605321",
   "metadata": {},
   "source": [
    "Let design our first neural network and train to (hopefully) solve this learning task. Here is the general pipeline:\n",
    "\n",
    "- Define the model using `tensorflow.keras.models.Sequential`. \n",
    "- Compile the model, defining (i) the _optimization_ procedure used to optimize its parameters, (ii) the loss we want to minimize, (iii) the metric we are interested in.\n",
    "- (Optionnal) Display a summary of your model, just to check it's number of parameters. \n",
    "- Train the model on the training set. \n",
    "- Test the actual performances of the model on the test set. \n",
    "\n",
    "To help you getting started quickly, we provide a \"minimal working example\" below. \n",
    "\n",
    "**Note:** `loss` and `metric` look similar, but basically the `loss` is what you _minimize_, e.g. cross entropy, while the `metric` is what you are interested in \"at the end of the day\" as a human being, such as the accuracy (which is way more interpretable than saying \"we reach a cross entropy of 0.134\"). \n",
    "In some cases (typically, for regression tasks), they can be the same (the MSE in both cases); but for classification tasks, they can differ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d4a4f1-62d7-4382-9ab1-cfb23a43b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a tensorflow neural network via keras\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e33a0-1982-4399-97b8-64895b93b3e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0659b-4cd9-42cd-9726-5d0566a20b9c",
   "metadata": {},
   "source": [
    "**Question 1:** Quickly describe this block of code. Intuitively, (i) what does `Sequential` mean? (ii) why the final layer is of dimension `10`? (You can check the documentation of `Flatten` and `Dense` layers [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) and [there](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2dab1-6fb4-4f20-9329-d03fb4d0e9f0",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee06bc5b-9a58-4b67-b405-831c425616a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c953cc-ba92-4f1b-a96e-76fde979765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we compile the model and plot a summary of it. \n",
    "\n",
    "model.compile(optimizer='SGD',  # the optimization procedure (here, Stochastic Gradient Descent)\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # The loss we minimize.\n",
    "              metrics=['accuracy'])  # the metric we are eventually interested in.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a4257-5bd9-49c7-aad8-cb1d80e670e0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23d620-272d-49be-8a51-37e5813636be",
   "metadata": {},
   "source": [
    "**Question 2:** \n",
    "- What does the parameter `from_logits=True` means in `SparseCategoricalCrossentropy`? We discussed it during the lectures, but if you don't remember you can check the documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy). \n",
    "- What is `activation='relu'` ? Are there other possible choices? What happen if we don't specify the activation (as we do for the last layer). You can check the documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/activations). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d19075-221a-4690-a62f-f48205d648ba",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511c68e-ad02-42f8-b03d-f5c55db87052",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90101beb-e404-4fe8-879f-04f64698d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train the model\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f1c533-11f6-4b2f-ba3d-6990de776edb",
   "metadata": {},
   "source": [
    "**Question 3:** \n",
    "- What is `epochs` here? \n",
    "- What does the counter `../1875` represent for each epoch?\n",
    "- Do you think that our model converged?\n",
    "- What's the final training accuracy of the model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562828f6-f6b9-4d9c-aa5c-9803dafaf28d",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8628776-41b2-4961-b61e-eb5142085132",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f15845-d5ba-4eff-bd9e-cc53d2cf0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we test our model. \n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20babf-347d-4de6-b3bd-a667104abaa5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428aa9f1-46b1-41f7-98f5-2f63646f26d1",
   "metadata": {},
   "source": [
    "**Question 4:** What's the test accuracy of your model? Does it overfit? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111c244-2307-42df-ac6e-1407b4f96ad7",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036092f8-033f-4f7a-bafd-2f3ccfd778b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e2bd9-857d-4625-854f-2409308208f2",
   "metadata": {},
   "source": [
    "We now go a bit deeper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66893e78-74ed-4d90-b434-ab0150891b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(x_test.shape[0])\n",
    "\n",
    "logit = model.predict(np.array([x_test[i]]))\n",
    "\n",
    "sm = tf.keras.layers.Softmax()(logit)\n",
    "\n",
    "pred = np.argmax(sm, axis=1)[0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ax = axs[0]\n",
    "ax.imshow(x_test[i], cmap='Greys')\n",
    "ax.set_title(\"True Label = %s, prediction %s\" %(y_test[i],pred))\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "ax = axs[1]\n",
    "ax.bar(np.arange(10), sm[0])\n",
    "ax.set_xticks(np.arange(10))\n",
    "ax.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ec0ff-2c38-40fb-a7c9-4d8ca41d86f3",
   "metadata": {},
   "source": [
    "**Question 5:** Interpret the above code (and output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c5200-4d5f-490f-baa8-bb2a36e192af",
   "metadata": {},
   "source": [
    "-- Write your comment there --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c06ca79-e0c5-446b-ab50-1f5759521af9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99619f24-6e45-4fcb-b6c4-da1ebc278d3d",
   "metadata": {},
   "source": [
    "**Question 6:** Design a similar architecture (still `Sequential` with `Dense` layers) but with a bit more layers (not too much or your laptop will struggle). Discuss the differences (training time, overfitting... anything you find interesting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0f474-3a35-496c-930d-d6a6317a206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353b775-da10-4d92-92a4-e35f21052569",
   "metadata": {},
   "source": [
    "-- Write some comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea30340-5278-4961-accd-07dd3f340485",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc9d32-db69-408e-9959-d53b5451bd66",
   "metadata": {},
   "source": [
    "## 2. The CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d1e29a-a080-4f2d-8492-42cb2c5a336d",
   "metadata": {},
   "source": [
    "At this point, you should be satisfied with your performances on MNIST. \n",
    "We will thus consider a more advanced dataset, the CIFAR10 dataset. \n",
    "It represents 10 different types of objects. \n",
    "Images are of size $32 \\times 32$ and are RGB-colored (Red Green Blue), so a given image is of shape `32 x 32 x 3` (three _channels_ for each color). \n",
    "\n",
    "This part of the lab is highly inspired from [this tutorial](https://www.tensorflow.org/tutorials/images/cnn).\n",
    "\n",
    "The labels are given as digits between `0` and `9`, and correspond to the different types of objects. \n",
    "We provide an array to do the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f55b6-6f49-4905-ae0a-8c374897ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion digit to name:\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86a44d-10ae-48fc-85bb-cb2fb5b95639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dbf9da-bf6a-4af0-9bb9-c34ebf61adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some investigation and visualization. \n",
    "print(\"Shape of training set:\", train_images.shape)\n",
    "print(\"Shape of test set:\", test_images.shape)\n",
    "\n",
    "i = np.random.randint(train_images.shape[0])  # random index in the training set\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "ax.imshow(train_images[i])\n",
    "ax.set_title(\"Image number %s representing a %s.\" %(i,class_names[train_labels[i][0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db66344-1d0a-4085-a296-41ca4c32ee2f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ef852-f64c-4478-98b4-c4bd192329f0",
   "metadata": {},
   "source": [
    "**Question 7:** Design a `Sequential` network made of `Dense` layers (with correct input and output shape), compile it, train and test it on this dataset. Comment the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e21256-ee34-43b1-9336-4f59753e0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54df82-66ea-406b-a7bf-eabc36bda1ac",
   "metadata": {},
   "source": [
    "-- Write comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdc2d01-8a7c-4688-8104-9369e46d4c4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb63f81-d59e-49fd-970f-54783697bc53",
   "metadata": {},
   "source": [
    "To improve our results on this harder dataset, we will now consider layers more tailored for image classification, namely [Convolutional layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D). \n",
    "\n",
    "We provide a Minimal Working Example below. \n",
    "\n",
    "**Note:** `Conv2D` layers are made to handle 2D images, so we do not need the `Flatten` layer first (as we did when we wanted to use `Dense` layers), but we should do it at the penultimate step to turn our \"image\" into a vector of logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbf9de-9c32-4bb7-9315-bd5ff3054ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(32, 32, 3)), \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ee3e7-b1d9-45e1-8301-67bab37198d4",
   "metadata": {},
   "source": [
    "We compile the model, using the [adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimizer, an improved version of the vanilla SGD, commonly used in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f02c0-1492-4bf8-82fd-05bfa3dd40d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9df108-3be5-41fd-b2d2-00f0a04c3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306c008-6355-4dda-9c00-c36808701df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now evaluate it. \n",
    "model.evaluate(test_images, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f7796b-f135-410f-92e0-a3b25f55248c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7d228-104b-4970-b5f1-b9022bfb8fb7",
   "metadata": {},
   "source": [
    "**Question 8:** Briefly comment the code/results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559008d1-02f8-4636-bfe9-4281178817a2",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f797e55-f6ce-4f4b-89e8-6cbbb14aadbc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90dc30-ba07-4807-b555-99df9d072a63",
   "metadata": {},
   "source": [
    "**Question 9:** Design a model that reach better performances. You should \"easily\" reach 75%, possibly up to 85% with such methods. To reach state-of-the-art results, you need more advanced techniques such as [ResNet](https://en.wikipedia.org/wiki/Residual_neural_network). If you beat 99.2%, you're officially world champion, congrats---but is starts being suspicious because [some labels are known to be **wrong**](https://franky07724-57962.medium.com/once-upon-a-time-in-cifar-10-c26bb056b4ce). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87399ba-a0c9-40b7-8330-6b3ebcd548e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0da13e-f022-4b85-a4cc-fa7bfddfd9b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e1ce5-7932-4ca0-bd2e-cbb25a372c9c",
   "metadata": {},
   "source": [
    "**Question 10 (optional):** Find a way to visualize the intermediate representations of your CNN. That is, if your CNN is encoding $F = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1(x)$, plot $f_\\ell \\circ \\dots \\circ f_1(x)$ for the intermediate $\\ell \\in \\{1,\\dots,L\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bfc4ef-daf3-4382-8bc2-aa581f01b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ddb20-58a2-4d2a-b357-bbbb546b98a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a03477-64db-488e-a17d-72f7c0d13022",
   "metadata": {},
   "source": [
    "## 3. Some other advantages of CNN over Dense layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9132ae-0fc9-4987-a469-5e6a9ca6b69b",
   "metadata": {},
   "source": [
    "As you can see, with a similar number of parameters, CNN are much better than naive Fully-connected NN when it comes to learn from _natural_ 2D images. \n",
    "\n",
    "Interestingly, they have some other advantages. \n",
    "\n",
    "In this section, we will go back to the MNIST dataset (which was seemingly reasonnably \"solved\" by a simple Fully-connected network). However, we will consider the context of _distribution shift_: when your **test data** are not exactly similar to the ones used at training time.\n",
    "This is a very important situation that occurs in practical applications: you train a model on clean data, but when deployed in \"real-life\", data are slightly different (e.g. some noise; not exactly the same population, etc.) and this can have dramatic consequences in terms of performances. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e4d1f-ac45-4dee-9099-d8abe43bef11",
   "metadata": {},
   "source": [
    "In this section, we will consider a simple noise model on test data: each pixel is augmented by a random uniform noise $\\epsilon \\sim \\mathcal{U}(0,\\sigma)$ for some $\\sigma$, cliped at $1$ (so that we stay with images having pixels values in $(0,1)$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea0d5d-04f1-4d3b-8b86-2a1fbd4d4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_corruption(images_set, sigma):\n",
    "    \"\"\"\n",
    "    Corrupt a set of image by adding a white noise of variance sigma. \n",
    "    \"\"\"\n",
    "    shape = images_set.shape\n",
    "    \n",
    "    corrupted_set = np.minimum(images_set + sigma * np.random.rand(*shape), 1)\n",
    "    \n",
    "    return corrupted_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2325ba1-35cf-49a4-8592-490cc2430b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset again, just to be sure\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize to make value between 0 and 1. Not necessary, but common practice. \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5b293-10a1-43a8-9871-1aafcb2a11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the \"corrupted\" test set.\n",
    "corrupted_test_set = image_corruption(x_test, sigma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b57476-a59f-4afe-a742-858b391f73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of a corrupted image.\n",
    "i = np.random.randint(corrupted_test_set.shape[0])  # sample random index\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(corrupted_test_set[i], cmap='Greys')\n",
    "ax.set_title(\"Corrupted image of a %s\" %y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f703369-beee-4a8a-a336-e8b0ee9938cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6074ff3-efa6-424b-b4c7-8f3edc9c943c",
   "metadata": {},
   "source": [
    "**Question 10:** Design `model_FC` and `model_CNN` (respectively a Fully-connected NN and a CNN) and train them on MNIST. They should both reach above 95% of _training_ accuracy, say (it's not central if one is slightly better than the other) ; but keep them simple as much as possible. Then test them on the `corrupted_test_set`.\n",
    "\n",
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774287b-2527-4232-8500-0c31086b5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here. \n",
    "\n",
    "model_FC = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8787a24-60c2-42e8-bcf2-2c2d95d1e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb3baf-384f-4f0a-89c8-349ee035378b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a37c9e-da62-498a-8e73-ab7b96368e16",
   "metadata": {},
   "source": [
    "# III. Regression and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dfd8e-bc60-4a27-aed5-4695ce8b37a9",
   "metadata": {},
   "source": [
    "The goal of this part of the lab session is to showcase an important technique in training machine learning models (and unfortunately not studied in details during the lectures): _regularization_. \n",
    "\n",
    "The key idea is the following one: we want a large class of models $\\{F(\\cdot, \\theta),\\ \\theta \\in \\Theta\\}$, but at the same time we want to prevent overfitting (which is more likely to occur with larger class of models which are more likely to interpolate training data exactly). For this, we will _regularize_ on $\\theta$, roughly saying \"if you want a complex model (more likely to overfit), you have to pay for it, so only do it if this is strictly necessary\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa066f1-873b-4750-86be-eb10c5bdfe58",
   "metadata": {},
   "source": [
    "**Polynomial regression:** For the sake of simplicity, we stick to a simple usecase: data and labels are simply in $\\R$, and we suspect a polynomial relation between the two, that is\n",
    "\n",
    "$$y = \\sum_{i=0}^d a_i x^i \\quad + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0,1)$ is some random gaussian noise. The key idea is that the degree $d$ is unknown, but say that we know that is should be lesser than $10$ for sure. Meanwhile, we want to learn the parameters $a_i$. \n",
    "\n",
    "We recall the following trick: if we let $\\mathbf{x} = (1, x, x^2,\\dots,x^d)$ and $\\theta = (a_0,a_1,\\dots,a_d)$, the relation above can be written\n",
    "\n",
    "$$y = \\theta \\cdot \\mathbf{x} + \\epsilon,$$\n",
    "\n",
    "so _polynomial regression_ is simply a _linear regression_ on the _augmented variable_ $\\mathbf{x}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f7db8-7f96-495f-83b3-3787340a5dbf",
   "metadata": {},
   "source": [
    "$\\bullet$ Let us first generate and plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d742be9b-70dc-4fcb-88da-a18a4dad9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = utils.generate_data_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb2097-a056-4c2d-b968-5bc40455ebb5",
   "metadata": {},
   "source": [
    "$\\bullet$ Now, we will train a natural Linear Regression using `scikit-learn` (more precisely, the module `sklearn.linear_model`, imported as `skl` in the first cell of the notebook), see [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) for the documentation. We use the `PolynomialFeatures()` preprocessing method also provided by `scikit-learn` [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "\n",
    "We provide the code below to save some time :-). Please **read it carefully** to understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8bd854-fd8b-4aee-8b65-4a923a3ca851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the model (as an object)\n",
    "model = skl.LinearRegression(fit_intercept=False)  # don't worry about this fit_intercept. \n",
    "\n",
    "# We define the maximum degree we consider.\n",
    "d_max = 10\n",
    "\n",
    "# We define the \"PolynomialFeatures\" object that help us to build augmented variables. \n",
    "PF = PolynomialFeatures(degree=d_max)\n",
    "\n",
    "# We build the augmented variables\n",
    "x_train_augmented = PF.fit_transform(x_train[:,None])\n",
    "\n",
    "# Now we use our LinearRegression to fit the augmented variable <==> polynomial regression.\n",
    "model.fit(x_train_augmented, y_train)\n",
    "\n",
    "# We can now check the coefficients of our trained model. We use a function from utils for better ploting results. \n",
    "utils.display_polynom(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f6d57-7197-463f-97f4-108fcd64d0b3",
   "metadata": {},
   "source": [
    "Though this polynomial looks weird, maybe it's the correct one. Let us look at its MSE (Mean Squared Error), which, we recall, is\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n (\\theta \\cdot \\mathbf{x}_i - y_i)^2. $$\n",
    "\n",
    "This quantity represents, as it name suggests, the average _squared_ error our model does. So a MSE of $16$ would mean \"on average, the square of the error we make is $16$\". \n",
    "To get a quantity easier to interpret, we take the square root of it (with the previous example, it yields an average error of $4$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6e102-b4ff-4060-95a1-a3c371b6b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the training loss. \n",
    "print(\"Training loss (square root MSE):\", np.mean((model.predict(x_train_augmented) - y_train)**2)**(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618f229-6d65-4c2a-8207-c6c18a7fa260",
   "metadata": {},
   "source": [
    "You should have a seemingly small training loss (I mean, the data should vary between $-30$ and $30$, so an average error lesser than $1$ for instance is quite good). \n",
    "\n",
    "But of course, what matter is the test loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f04c23-0948-4124-8a56-4f5d9493060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the training loss. \n",
    "print(\"Test loss (MSE):\", np.mean((model.predict(PF.fit_transform(x_test[:,None])) - y_test)**2)**(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb982f-4074-4535-ad19-cc17bb9ffbf2",
   "metadata": {},
   "source": [
    "It is quite likely that the test loss is much higher than the training loss... What happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac661599-bfd1-492c-8beb-1a04c9a99fb1",
   "metadata": {},
   "source": [
    "To get a visual idea of how our model behaves in practice, as we are in 1D, we can simply evaluate it on \"all points in $(-3,3)$\" (up to a discretization). \n",
    "Recall that, from `sklearn` perspective, our model is a `LinearRegression` on augmented data obtained through `PolynomialFeatures`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c8596-8f61-41d5-aedb-d680b93273c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-3.5, 3.5, 500)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_train, y_train, label='Training set', c='blue')\n",
    "ax.scatter(x_test, y_test, label='Training set', c='orange')\n",
    "ax.plot(t, model.predict(PolynomialFeatures(degree=d_max).fit_transform(t[:,None])), c='red', label='Our model')\n",
    "ax.set_ylim(min(np.min(y_train), np.min(y_test)) - 5, max(np.max(y_train), np.max(y_test)) + 5)\n",
    "ax.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f74be0-4190-4281-95ee-141a4d5bd0a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faff17ec-0093-4cff-89ef-5b530030d3a4",
   "metadata": {},
   "source": [
    "**Question 1:** What conclusion can you reach from this plot and previous observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145fa1a-5c61-48a7-b457-0baaea007878",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db949f-73b2-4f67-b8cf-6b3c748a9b02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ede77f-a58d-4171-ad47-038304d7f451",
   "metadata": {},
   "source": [
    "We definitely must improve on this. Intuitively, \n",
    "- dramatic overfitting is due to the polynomial encoded by the model varying too much,\n",
    "- Variation in the polynomial are due to large (in absolute value) coefficients (why?). That is, $\\theta$ with high norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a216b8-0c3d-4ba0-9b08-4d28e31cb783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the norm of our Theta:\n",
    "np.linalg.norm(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d73b6-ebfa-4c06-9635-a30b0ee10f70",
   "metadata": {},
   "source": [
    "From this observation, a natural attempt to mitigate overfitting is to **control** the coefficients of our polynomial, that is _penalize_ parameter $\\theta$ with high norm (in $L^2$ or $L^1$) sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147c21c-8634-4c65-b920-9f270c4e3379",
   "metadata": {},
   "source": [
    "In a nutshell, instead of only minimizing the MSE in $\\theta$, we will instead minimize the following loss:\n",
    "\n",
    "$$L : \\theta \\mapsto \\frac{1}{n} \\sum_{i=1}^n (\\theta \\cdot \\mathbf{x}_i  - y_i)^2 + \\alpha \\|\\theta\\|^2,$$\n",
    "\n",
    "This training procedure is called a **Ridge regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcaa34-ff90-4670-8e04-727f4125d85e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40a482-fb81-4a44-b660-fcffb870e8d8",
   "metadata": {},
   "source": [
    "**Question 2:** Is this loss function convex? Is there a closed form for the optimal $\\theta$? What is the role of the parameter $\\alpha \\in \\R$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b14c75-7bb7-4025-8e3e-5e5db4c99cd0",
   "metadata": {},
   "source": [
    "-- Write your comments here --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420ec99-df99-4a8a-873f-583e3db58318",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2e56bf-158c-450a-bd24-3fd433fe6f0b",
   "metadata": {},
   "source": [
    "**Question 3:** Adapting the previous code, build a `Ridge` model (representing a Ridge regression, see [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)), train it, check the training and test loss (for the standard MSE, not the Ridge loss), plot the model, the norm of the resulting $\\alpha$, and comment the results. \n",
    "\n",
    "_Note:_ Try for different values of `alpha`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df58f19-03ea-4171-b2a0-8066732f77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
